{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Практическое задание 4 \n",
    "\n",
    "# Мультиязычный тематический поиск\n",
    "\n",
    "## курс \"Математические методы анализа текстов\"\n",
    "\n",
    "\n",
    "### ФИО: <Волынцев Дмитрий Александрович>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Введение\n",
    "\n",
    "### Постановка задачи:\n",
    "\n",
    "В этом задании вам предстоит сделать свою небольшую мультиязычную поисковую систему. Проще всего строить мультиязычные системы, имея \"параллельные\" данные: словари или корпуса параллельных текстов. \n",
    "\n",
    "В задании необходимо, имея англо-русскую и агло-испанскую коллекции, обучить модель поиска модель поиска испанских текстов по русским.\n",
    "\n",
    "Решение этого задания будет основано на тематическом моделировании, а именно подходе аддитивной регуляризации.\n",
    "\n",
    "### Библиотеки\n",
    "\n",
    "Для этого задания вам понадобятся следующие библиотеки:\n",
    " - [bigartm](http://bigartm.org/)\n",
    " - [pymorphy2](https://pymorphy2.readthedocs.io/en/latest/)\n",
    " - [nltk](http://www.nltk.org/)\n",
    "\n",
    "\n",
    "### Данные\n",
    "\n",
    "Данные — записи выступлений конференции TED Talks на трёх языках. \n",
    "\n",
    "Все данные содержатся в архиве `ted_collection`. В папке содержится три подпапки `/en`, `/ru` и `/es`, каждая из которых соответствует коллекции для отдельного языка. Папка `parallel_info`  содержит информацию о связях документов между коллекциями. Файл `titles_file.json` содержит информацию о заголовках документов английской коллекции.\n",
    "\n",
    "Ссылка для скачивания данных: [ссылка на гугл диск](https://drive.google.com/file/d/1B3kDfISvWnVpEet_CDa6oLNp028mEak-/view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Импорт важных библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymorphy2Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
      "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
      "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
      "Collecting dawg-python>=0.7.1\n",
      "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
      "Collecting docopt>=0.6\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "Building wheels for collected packages: docopt\n",
      "  Building wheel for docopt (setup.py): started\n",
      "  Building wheel for docopt (setup.py): finished with status 'done'\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13709 sha256=b33c5ad44e792cacd8e09301450758ffefd44a67468c48bfe5b7e82704c87f5d\n",
      "  Stored in directory: c:\\users\\8c33~1\\appdata\\local\\pip\\cache\\wheels\\72\\b0\\3f\\1d95f96ff986c7dfffe46ce2be4062f38ebd04b506c77c81b9\n",
      "Successfully built docopt\n",
      "Installing collected packages: pymorphy2-dicts-ru, dawg-python, docopt, pymorphy2\n",
      "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n"
     ]
    }
   ],
   "source": [
    "pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting protobuf==3.0.0Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: tensorflow 2.1.0 has requirement protobuf>=3.8.0, but you'll have protobuf 3.0.0 which is incompatible.\n",
      "ERROR: tensorboard 2.1.0 has requirement protobuf>=3.6.0, but you'll have protobuf 3.0.0 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading protobuf-3.0.0-py2.py3-none-any.whl (342 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\дмитрий в\\appdata\\roaming\\python\\python37\\site-packages (from protobuf==3.0.0) (46.1.3)\n",
      "Requirement already satisfied: six>=1.9 in c:\\users\\дмитрий в\\appdata\\roaming\\python\\python37\\site-packages (from protobuf==3.0.0) (1.14.0)\n",
      "Installing collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.11.4\n",
      "    Uninstalling protobuf-3.11.4:\n",
      "      Successfully uninstalled protobuf-3.11.4\n",
      "Successfully installed protobuf-3.11.3\n"
     ]
    }
   ],
   "source": [
    "pip install protobuf==3.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement artm (from versions: none)\n",
      "ERROR: No matching distribution found for artm\n"
     ]
    }
   ],
   "source": [
    "pip install bigartm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'artm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-6432ffbffd39>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# тематическое моделирование\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0martm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# change log style for artm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'artm'"
     ]
    }
   ],
   "source": [
    "# считывание\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "\n",
    "# предобработка\n",
    "import pymorphy2\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "# тематическое моделирование\n",
    "import artm\n",
    "\n",
    "# change log style for artm\n",
    "lc = artm.messages.ConfigureLoggingArgs()\n",
    "lc.minloglevel = 3\n",
    "lib = artm.wrapper.LibArtm(logging_config=lc)\n",
    "\n",
    "# визуализация\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# прочее \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Несколько важных промежуточных функций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузка коллекции\n",
    "from lab4_utils import load_collection\n",
    "\n",
    "# загрузка информации о параллельных документах\n",
    "from lab4_utils import load_parallel_documents_info\n",
    "\n",
    "# запись vowpal wabit файла специального формата\n",
    "from lab4_utils import write_vw_lab4\n",
    "\n",
    "# подсчёт позиции в выдаче переводов текстов\n",
    "from lab4_utils import get_indexes_of_relevant_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предобработка данных (3 балла)\n",
    "\n",
    "Перед тем как начать моделировать, необходимо предобработать данные. \n",
    "\n",
    "Заметим, что для английского языка некоторые этапы обработки не оказывают сильного влияния на модель (например, лемматизация). В русском языке одному слову может соответствовать огромное количество различных форм, поэтому без лемматизации невозможно получить модель с хорошим качеством."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Предобработка русских текстов\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считываем коллекцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RU_DATA_PATH = 'ted_collection/ru'\n",
    "ru_collection = load_collection(RU_DATA_PATH, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведите полностью несколько документов. Подумайте, какую информацию из них можно удалить на этапе предобработки, не ухудшив качество решения задачи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помимо удаления специфичной информации, вам необходимо провести для русского языка следующие шаги предобработки :\n",
    "\n",
    "1. Приведение документов к нижнему регистру.\n",
    "\n",
    "2. Удаление всех символов кроме букв.\n",
    "    1. Для некоторых способов выделения коллокаций (см. бонусную часть), может быть полезна информация о знаках препинания. Также она полезна при необходимости строить синтаксический разбор предложения.\n",
    "    2. Вам может помочь функция sub из библиотеки re.\n",
    "3. Токенизация документов.\n",
    "    1. Воспользуйтесь стандартным методом .split, функцией split из библиотеки re или одним из токенайзеров библиотеки nltk.\n",
    "4. Лемматизация документов.\n",
    "    1. Воспользуйтесь библиотекой pymorphy2\n",
    "    2. Шаги 3 и 4 можно выполнить вместе, воспользовавшись библиотекой mytem (или её обёрткой на python pymystem)\n",
    "5. Удаление стоп-слов\n",
    "    1. Базовый список стоп слов можно получить из модуля nltk.corpus\n",
    "\n",
    "После выполнения всех шагов сохраните результат в словарь аналогичный ru_collection (ключи - названия файлов, значения - предобработанный документ в формате str).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проанализируйте несколько предобработанных документов. Отметьте, какие неточности работы алгоритмов вы заметили, и как они могут повлиять на итоговую модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Предобработка английских текстов\n",
    "\n",
    "Считываем коллекцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_DATA_PATH = 'ted_collection/en'\n",
    "en_collection = load_collection(EN_DATA_PATH, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведите все этапы предобработки для английского языка (шаг 4 опционален, можно использовать WordNetLemmatizer из nltk):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Предобработка испанских текстов \n",
    "\n",
    "Считываем коллекцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_DATA_PATH = 'ted_collection/es'\n",
    "es_collection = load_collection(ES_DATA_PATH, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведите все этапы предобработки для испанского языка (шаг 4 опционален, можно использовать SpanishStemmer из nltk). \n",
    "\n",
    "**Замечание.** Регулярное выражение \\w из библиотеки re позволяет выделять буквы (в том числе буквы испанского алфавита)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Дополнительная предобработка\n",
    "\n",
    "Библиотека BigARTM имеет собственный формат документов для обработки, называемый батчами. Самый простой способ создать батчи из коллекции файлов - сконвертировать в батчи vowpal wabbit файл с коллекцией (https://github.com/VowpalWabbit/vowpal_wabbit/wiki/Input-format). \n",
    "\n",
    "Тематические модели работают с мешком слов, поэтому в vowpal wabbit файле можно не хранить информацию о порядке слов в документе.\n",
    "\n",
    "Чтобы обучить мультиязычную модель, мы будем использовать апарат модальностей тематической модели. Каждому языку будет соответствовать своя модальность (`@english`, `@russian` и `@spanish` в vowpal wabbit файле). \n",
    "\n",
    "Для экспериментов нам понадобится два файла. Один будет содержать информацию о параллельных документов, а другой нет. В втором файле каждая строчка соответствует конкретному документу. В первом файле, если у документа есть параллельный, он будет располагаться на этой же строчке в рамках другой модальности.\n",
    "\n",
    "Весь код для сохранения файла в vowpal wabbit формате уже написан, вам только необходимо правильно воспользоваться функцией. В частности, проследите, чтобы в ваших документах не содержалось символов ':', '|' и '@'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В файлах en_ru_match.txt и en_es_match.txt содержится информация о том, какие документы являются параллельными. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_ru_parallel_docs = load_parallel_documents_info('ted_collection/en_ru_match.txt')\n",
    "en_es_parallel_docs = load_parallel_documents_info('ted_collection/en_es_match.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользуйтесь функцией write_vw_lab4, чтобы сохранить данные в нужном формате:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vowpal wabbit файлы с коллекцией\n",
    "DATA_PATH_PARALLEL = 'ted_collection/collection_parallel.vw'\n",
    "DATA_PATH_MONO = 'ted_collection/collection_mono.vw'\n",
    "\n",
    "write_vw_lab4(\n",
    "    DATA_PATH_PARALLEL,\n",
    "    # your en cleaned collection dict,\n",
    "    # your ru cleaned collection dict,\n",
    "    # your es cleaned collection dict,\n",
    "    en_ru_parallel_docs,\n",
    "    en_es_parallel_docs,\n",
    "    use_parallel_info=True\n",
    ")\n",
    "\n",
    "\n",
    "write_vw_lab4(\n",
    "    DATA_PATH_MONO,\n",
    "    # your en cleaned collection dict,\n",
    "    # your ru cleaned collection dict,\n",
    "    # your es cleaned collection dict,\n",
    "    en_ru_parallel_docs,\n",
    "    en_es_parallel_docs,\n",
    "    use_parallel_info=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Базовая моноязычная тематическая модель (2 балла)\n",
    "\n",
    "Теорию тематического моделирования можно узнать из соответствующей лекции курса.\n",
    "\n",
    "Научиться пользоваться bigartm легче всего по гайду из документации [ссылка](http://docs.bigartm.org/en/stable/tutorials/python_userguide/index.html).\n",
    "\n",
    "#### Подготовка к моделированию\n",
    "\n",
    "Чтобы преобразовать полученный vowpal wabbit файл в батчи, можно воспользоваться стандартным классом BatchVectorizer. Объект этого класса принимает на вход адрес папки с батчами или файл vowpal wabbit, а затем подаётся на вход для обучения методам. В случае, если входные данные не являются батчами, он создаёт их и сохраняет на диск для последующего быстрого использования.\n",
    "\n",
    "В этой части экспериментов, вам предлагается построить моноязычную тематическую модель только для английского языка, поэтому для обучения используйте файл DATA_PATH_MONO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# папка с батчами\n",
    "BATCHES_PATH_MONO = ## your code here ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# если BATCHES_PATH_MONO пуста, батчи будут созданы из файла в DATA_PATH_MONO\n",
    "# иначе использовать BATCHES_PATH_MONO\n",
    "if len(glob.glob(os.path.join(BATCHES_PATH_MONO + '/*.batch'))) < 1:\n",
    "    batch_vectorizer_mono = artm.BatchVectorizer(data_path=DATA_PATH_MONO, \n",
    "                                                 data_format='vowpal_wabbit',\n",
    "                                                 target_folder=BATCHES_PATH_MONO)\n",
    "else:\n",
    "    batch_vectorizer_mono = artm.BatchVectorizer(data_path=BATCHES_PATH_MONO,\n",
    "                                                 data_format='batches')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Словарь – это объект BigARTM, содержащий информацию о коллекции (словарь коллекции, различные величины и счётчики, связанные со словами). Создать словарь можно на основе папки с батчами. Затем собранный словарь можно сохранять на диск и позже подгрузить вновь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = artm.Dictionary()\n",
    "dictionary.gather(data_path=BATCHES_PATH_MONO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Словарь в том числе отвечает за то, на какие токены будет настраиваться модель. Редкие слова не оказывают влияние на модель, поэтому их можно удалить используя метод .filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_DF = 5\n",
    "\n",
    "dictionary.filter(min_df=MIN_DF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обучение модели\n",
    "\n",
    "Пришло время приступить к моделированию! Начнём с простой одноязычной модели PLSA. Будем учитывать только модальность @english в документах коллекции. Так как коллекция небольшая, используйте небольшое число тем 30-50.\n",
    "\n",
    "Параметр theta_columns_naming='title' отвечает за именование документов лейблами из vowpal wabbit формата при получении матрицы $\\Theta$ (иначе они будут нумероваться в порядке появления в коллекции)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = artm.ARTM(num_topics=30,\n",
    "                  num_processors=7,\n",
    "                  theta_columns_naming='title',\n",
    "                  show_progress_bars=True,\n",
    "                  class_ids={'@english':1})\n",
    "\n",
    "model.initialize(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Класс artm.ARTM поддерживает различные встроенные метрики качества. Добавьте метрики измерения перплексии, разреженности $\\Phi$, разреженности $\\Theta$ и счётчик топ слов. Не забудьте, что метрики должны соответствовать только модальности @english!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведите обучение модели с помощью метода fit_offline. Подберите необходимое число операций для сходимости в зависимости от значения перплексии:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведите график изменения перплексии в зависимости от итерации алгоритма, чтобы мы знали, что ваш алгоритм точно сошёлся :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Интерпретация результатов\n",
    "\n",
    "Основная особенность тематических моделей — интерпретируемость получаемых матриц $\\Phi$ и $\\Theta$. С помощью $\\Phi$ для каждой темы можно узнать топ-слова, а с помощью $\\Theta$ для каждой темы можно узнать топ-документы. \n",
    "Для того, чтобы получить матрицу $\\Theta$, используйте метод .transform.\n",
    "\n",
    "Для каждой темы выведите топ её слов ($\\geq 20$) и топ заголовкой её документов ($\\geq 5$). Попробуйте интерпертировать полученные темы, действительно ли темы получаются осмысленными?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_to_title = {}\n",
    "with open('ted_collection/titles_file.json') as f:\n",
    "    for line in f:\n",
    "        file_name_to_title.update(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# А здесь ваш текст про интерпретацию модели :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Мультиязычная тематическая модель (5 баллов)\n",
    "\n",
    "В этом пункте задания вам будет необходимо обучить мультиязычную тематическую модель.\n",
    "\n",
    "В данном пункте вы будете реализовывать модель ML-TD (MultiLingual Parallel).\n",
    "\n",
    "* каждый язык — отдельная модальность\n",
    "* $\\theta_{td}$ — общая для всех параллельных документов\n",
    "\n",
    "Таким образом на обучении все параллельные документы записываются в одну строку в vowpal wabbit файле.\n",
    "\n",
    "Оценивать качество модели мы будем на задаче поиска перевода текста. Вам будет необходимо оценить качество трёх переводов: с русского на английский, с испанского на английский и с русского на испанский.\n",
    "\n",
    "Поиск документов будет устроен следующим образом. Будем для документа d на языке A считать близости со всеми документами на языке B и ранжировать документы языка B по этой близости. Для каждого документа посчитаем позицию истинного перевода документа в выдаче. Итоговая метрика — медиана или среднее таких позиций по всем документам."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выделим множества документов, для которых не было известно информации об их переводе:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_es_parallel_docs_test = load_parallel_documents_info('ted_collection/ru_es_match_test.txt', verbose=False)\n",
    "ru_en_parallel_docs_test = load_parallel_documents_info('ted_collection/ru_en_match_test.txt', verbose=False)\n",
    "es_en_parallel_docs_test = load_parallel_documents_info('ted_collection/es_en_match_test.txt', verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведите подготовительный этап (создание батчей и словарей) для мультиязычной коллекции DATA_PATH_PARALLEL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучите мультиязычную модель и проинтерпертируйте полученные темы. Если вы всё сделали правильно, то топ-слова различных языков для одной темы должны получиться достаточно похожими."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестируйте качество на трёх задачах перевода. Добейтесь хорошего качества (медиана позиции в выдаче ~ 0, среднее ~ 10). Получить положение в выдаче переводов текстов вам поможет функция get_indexes_of_relevant_documents из модуля lab4_utils. Для того, чтобы всё работало корректно (на тесте не должна быть известна информация о параллельности документов), подавайте в качестве theta результат model.transform(batch_vectorizer_mono).\n",
    "\n",
    "Возможные способы улучшения:\n",
    "* изменять веса модальностей\n",
    "* изменять количество тем, количество итераций обучения\n",
    "* добавлять регуляризаторы (см. бонусную часть)\n",
    "* изменять метрику для поиска ближайших документов\n",
    "* добавлять шаги в предобработку (выделение колокаций)\n",
    "\n",
    "**В первую очередь** рекомендуется подобрать количество тем (в данной задаче хорошо работает небольшое число тем - несколько десятков) и веса модальностей.\n",
    "\n",
    "За нетривиальные подходы могут быть начислены дополнительные бонусные баллы!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведите анализ ошибок. На каких документах модель отработала лучше всего, на каких хуже всего? Как вы думаете почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Здесь ваш текст про ошибки модели :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бонусная часть\n",
    "\n",
    "#### Добавление фоновых тем (до 1 балла)\n",
    "\n",
    "Основной инструмент улучшения качества тематической модели — регуляризация. Выделите часть тем модели как фоновые. Сглаживайте $\\Theta$ для фоновых тем, разреживайте для предметных.\n",
    "\n",
    "Проинтерпретируйте результаты. Фоновые темы должны иметь в качество топ-слов слова фоновой лексики! Удалось ли с помощью введения фоновых тем повысить качество модели?\n",
    "\n",
    "#### Модальность n-грамм (до 2 баллов)\n",
    "\n",
    "Для каждого языка добавьте дополнительную модальность n-грамм. n-граммы можно выделить, например, с помощью пакета Phrases из модуля Gensim. Как отразилось добавление новой модальности на интерпретируемости модели? Удалось ли с помощью введения n-грамм повысить качество модели?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

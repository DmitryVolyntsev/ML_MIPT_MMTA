Este es Lee Sedol. Lee Sedol es uno de los mejores jugadores de Go del mundo. Y está teniendo lo que mis amigos de Silicon Valley llaman un momento "¡Bendito Dios!". (Risas) Un momento en el que nos damos cuenta de que la IA está avanzando mucho más rápido de lo que esperábamos. Los humanos han perdido en el tablero de Go. ¿Y en el mundo real? Bueno, el mundo real es mucho más grande y complicado que el tablero de Go. Es mucho menos visible. Pero sigue siendo un problema de decisión. Y si pensamos en algunas de las tecnologías que están por venir Noriko [Arai] mencionó que las máquinas aún no saben leer, al menos no comprendiendo, pero lo harán, y cuando eso suceda, poco después las máquinas habrán leído todo lo que la raza humana ha escrito. Eso permitirá a las máquinas, junto a su habilidad mirar más allá de lo que pueden los humanos, como ya hemos visto en el Go, si también tienen acceso a más información, serán capaces de tomar mejores decisiones en el mundo real que nosotros. ¿Es eso bueno? Bueno, espero que sí. Toda nuestra civilización, todo lo que valoramos, se basa en nuestra inteligencia. Y si tuviéramos acceso a mucha más inteligencia, entonces no existirían límites para lo que la raza humana pueda hacer. Y creo que este podría ser, como han dicho algunos, el mayor acontecimiento de la historia de la humanidad. Entonces, ¿por qué la gente afirma cosas como esta? Que la inteligencia artificial podría significar el fin de la raza humana. ¿Es esto algo nuevo? ¿Se trata solo de Elon Musk y Bill Gates y Stephen Hawking? En realidad, no. Esta idea no es nueva. He aquí una cita: "Incluso si pudiéramos mantener las máquinas en una posición servil, por ejemplo, desconectándolas en momentos estratégicos" —volveré a esa idea de "quitar la corriente" más adelante— "deberíamos, como especie, sentirnos humillados". ¿Quién dijo esto? Este es Alan Turing, en 1951. Alan Turing, como Uds. saben, es el padre de la informática y en muchos sentidos también el padre de la IA. Así que si pensamos en este problema, el problema de crear algo más inteligente que tu propia especie, podríamos llamar a esto "el problema del gorila". Porque los antepasados de los gorilas hicieron esto hace unos millones de años, y ahora podríamos preguntar a los gorilas: ¿Fue una buena idea? Aquí están, reunidos para discutir si fue una buena idea, y pasado un tiempo concluyen que no. Fue una idea terrible. Nuestra especie está en apuros. De hecho, pueden ver la tristeza existencial en sus ojos. (Risas) Así que esta sensación mareante de que crear algo más inteligente que tu propia especie tal vez no sea buena idea... ¿Qué podemos hacer al respecto? Bueno, nada en realidad, excepto dejar de hacer IA. Y por todos los beneficios que he mencionado y porque soy un investigador de IA, no voy a tomar eso. Sin duda quiero seguir creando IA. Así que necesitamos precisar el problema un poco más. ¿Cuál es el problema? ¿Por qué tener mejor IA puede ser una catástrofe? Aquí hay otra cita: "Más nos vale estar seguros de que el propósito que introducimos en la máquina es el que de verdad deseamos". Esto fue dicho por Norbert Wiener en 1960, poco después de ver a uno de los primeros sistemas de aprendizaje aprender a jugar a las damas mejor que su creador. Pero esto podría haberlo dicho de igual modo el Rey Midas. El Rey Midas dijo, "Deseo que todo lo que toque se convierta en oro". Y obtuvo justo lo que pidió. Fue el propósito que introdujo en la máquina, por así decirlo. Y luego su comida, su bebida y sus familiares se convirtieron en oro y murió miserable y muerto de hambre. Así que llamaremos a esto "el problema del rey Midas", el de indicar un objetivo que no está realmente alineado con lo que de verdad queremos. En términos modernos, lo llamamos "el problema de alineación de valor". Introducir un objetivo equivocado no es la única parte del problema. Hay otra parte. Al introducir un objetivo en una máquina incluso algo tan simple como "Trae el café", la máquina se dice a sí misma, "¿Cómo podría fallar yendo a buscar el café? Alguien podría desconectarme. Vale, debo tomar medidas para evitarlo. Desactivaré mi interruptor de 'apagado'. Haré cualquier cosa para protegerme de interferencias con este objetivo que me han dado. Así que esta persecución obsesiva de un modo muy defensivo para lograr un objetivo que no está alineado con los verdaderos objetivos de la raza humana... ese es el problema al que nos enfrentamos. Y de hecho esa es la lección más valiosa de esta charla. Si quieren recordar una cosa es que no se puede ir a buscar el café si se está muerto. (Risas) Es muy simple. Solo recuerden eso. Repítanlo tres veces al día. (Risas) Y de hecho, este es el mismo argumento de "2001: [Una odisea del espacio]". HAL tiene un objetivo, una misión, que no está alineada con los objetivos de los humanos, y eso conduce a este conflicto. Por suerte HAL no es superinteligente. Es bastante inteligente, pero llegado el momento, Dave lo supera y logra apagarlo. Pero tal vez no tengamos tanta suerte. Entonces, ¿qué vamos a hacer? Estoy tratando de redefinir la IA para alejarnos de esta noción clásica de máquinas que persiguen objetivos de manera inteligente. Hay tres principios implicados. El primero es un principio de altruismo, por así decirlo, el único objetivo del robot es maximizar la realización de los objetivos humanos, de los valores humanos. Y por valores aquí no me refiero a valores sentimentales o de bondad. Solo quiero decir aquello más similar a la vida que un humano preferiría. Y esto viola la ley de Asimov de que el robot debe proteger su propia existencia. No tiene ningún interés en preservar su existencia en absoluto. La segunda ley es una ley de humildad, digamos. Y resulta muy importante para que los robots sean seguros. Dice que el robot no sabe cuáles son esos valores humanos, así que debe maximizarlos, pero no sabe lo que son. Lo cual evita el problema de la búsqueda obsesiva de un objetivo. Esta incertidumbre resulta crucial. Claro que para sernos útiles, deben tener alguna idea de lo que queremos. Obtiene esa información sobre todo observando elecciones humanas, para que nuestras propias decisiones revelen información sobre lo que nosotros preferimos para nuestras vidas. Estos son los tres principios. Veamos cómo se aplica a esta cuestión de "apagar la máquina", como sugirió Turing. He aquí un robot PR2. Es uno que tenemos en nuestro laboratorio, y tiene un gran botón rojo de 'apagado' en la parte posterior. La pregunta es: ¿Va a dejar que lo apaguen? Si lo hacemos a la manera clásica, le damos el objetivo de traer el café. "Debo traer el café. No puedo traer el café si estoy muerto". Obviamente el PR2 ha escuchado mi charla, y por tanto, decide "Debo inhabilitar mi botón de 'apagado'". "Y probablemente electrocutar al resto de personas en el Starbucks que podrían interferir". (Risas) Así que esto parece ser inevitable, ¿verdad? Este tipo de error parece ser inevitable, y sucede por tener un objetivo concreto, definido. Entonces, ¿qué pasa si la máquina no tiene claro el objetivo? Bueno, razona de una manera diferente. Dice, "El humano podría desconectarme, pero solo si hago algo malo. No tengo claro lo que es malo pero sé que no quiero hacerlo". Ahí están el primer y el segundo principio. "Así que debería dejar que el humano me desconecte". De hecho se puede calcular el incentivo que tiene el robot para permitir que el humano lo apague. Y está directamente ligado al grado de incertidumbre sobre el objetivo subyacente. Y entonces cuando la máquina está apagada, el tercer principio entra en juego. Aprende algo sobre los objetivos que debe perseguir, porque aprende que lo que hizo no estaba bien. De hecho, podemos, con el uso adecuado de los símbolos griegos, como suelen hacer los matemáticos, podemos probar un teorema que dice que tal robot es probablemente beneficioso para el humano. Se está demostrablemente mejor con una máquina que se diseña de esta manera que sin ella. Este es un ejemplo muy simple, pero este es el primer paso en lo que estamos tratando de hacer con IA compatible con humanos. Ahora, este tercer principio, es probablemente el que está haciendo que se rasquen la cabeza. Probablemente piensen: "Yo me comporto mal. No quiero que mi robot se comporte como yo. Me escabullo en mitad de la noche y tomo cosas de la nevera, hago esto y hago aquello". Hay todo tipo de cosas que no quieres que haga el robot. Pero lo cierto es que no funciona así. Solo porque uno se comporte mal no significa que el robot vaya a copiar su comportamiento. Va a entender sus motivaciones y tal vez a ayudarle a resistirlas, si es apropiado. Pero sigue siendo difícil. Lo que estamos tratando de hacer, de hecho, es permitir que las máquinas predigan para cualquier persona y para cualquier vida posible que podrían vivir, y las vidas de todos los demás lo que preferirían. Y hay muchas, muchas dificultades ligadas a hacer esto. No espero que vaya a resolverse pronto. Las verdaderas dificultades, de hecho, somos nosotros. Como ya he mencionado, nos comportamos mal. De hecho, algunos de nosotros somos francamente desagradables. Como he dicho, el robot no tiene que copiar el comportamiento. El robot no tiene ningún objetivo propio. Es puramente altruista. Y no está diseñado solo para satisfacer los deseos de una persona, el usuario, sino que tiene que respetar las preferencias de todos. Así que puede lidiar con cierta cantidad de maldad, e incluso puede entender que su maldad, por ejemplo... Ud. puede aceptar sobornos como controlador de pasaportes porque necesita alimentar a su familia y que sus hijos vayan a la escuela. Puede entender eso; no significa que vaya a robar. De hecho, solo le ayudará a que sus hijos vayan al colegio. También estamos limitados computacionalmente. Lee Sedol es un jugador brillante de Go, pero aun así perdió. Si nos fijamos en sus acciones, tomó una decisión que le hizo perder. Eso no significa que él quisiera perder. Así que para entender su comportamiento, en realidad tenemos que invertir, a través de un modelo cognitivo humano que incluye nuestras limitaciones computacionales, y se trata de un modelo muy complicado. Pero es algo en lo que podemos trabajar para comprender. Puede que la parte más difícil, desde mi punto de vista como investigador de IA, es el hecho de que hay muchos de nosotros, con lo cual la máquina tiene que sopesar las preferencias de mucha gente diferente. Hay diferentes maneras de hacer eso. Economistas, sociólogos, filósofos morales han comprendido esto y estamos buscando colaboración de manera activa. Vamos a ver lo que sucede cuando esto se hace mal. Ud. puede estar hablando, por ejemplo, con su asistente personal inteligente que podría estar disponible dentro de unos años. Piensen en Siri con esteroides. Siri dice "Su esposa llamó para recordarle la cena de esta noche". Por supuesto, lo había olvidado. ¿Qué cena? ¿De qué está hablando? "Su 20 aniversario, a las 7pm". "No puedo, me reúno con el secretario general a las 7:30. ¿Cómo ha podido suceder esto?". "Bueno, le advertí, pero ignoró mi recomendación". "¿Qué voy a hacer? No puedo decirles que estoy demasiado ocupado". "No se preocupe, he hecho que su avión se retrase". (Risas) "Algún tipo de error en el ordenador". (Risas) "¿En serio? ¿Puede hacer eso?". "Le envía sinceras disculpas y espera poder conocerle mañana para el almuerzo". (Risas) Así que los valores aquí... aquí hay un pequeño fallo. Claramente está siguiendo los valores de mi esposa que son "esposa feliz, vida feliz". (Risas) Podría suceder al revés. Podría llegar a casa tras un duro día de trabajo, y el ordenador dice "¿Un día duro?". "Sí, ni tuve tiempo de almorzar". "Debe tener mucha hambre". "Me muero de hambre, sí, ¿podría preparar algo de cena?". "Hay algo que necesito decirle". (Risas) "Hay humanos en Sudán del Sur más necesitados que Ud.". (Risas) "Así que me voy, hágase su propia cena". (Risas) Así que tenemos que resolver estos problemas, y tengo ganas de trabajar en ellos. Hay razones para ser optimistas. Una razón es que hay gran cantidad de datos Recuerden, leerán todo lo que la raza humana ha escrito. La mayoría de lo que escribimos trata sobre humanos haciendo cosas y cómo estas molestan a otras personas. Así que hay muchos datos de los que aprender. También hay un fuerte incentivo económico para que esto funcione bien. Imagine que su robot doméstico está en casa Ud. llega tarde del trabajo, el robot tiene que dar de comer a los niños, los niños tienen hambre y no hay nada en la nevera. Y el robot ve al gato. (Risas) Y el robot no ha aprendido del todo bien la función del valor humano por lo que no entiende que el valor sentimental del gato supera el valor nutricional del gato. (Risas) Entonces, ¿qué pasa? Bueno, sucede lo siguiente: "Robot desquiciado cocina a un gatito para la cena familiar". Ese único incidente acabaría con la industria de robots domésticos. Así que hay un gran incentivo para hacer esto bien. mucho antes de llegar a las máquinas superinteligentes. Así que para resumir: Estoy intentando cambiar la definición de IA para que tengamos máquinas demostrablemente beneficiosas. Y los principios son: Máquinas que son altruistas, que desean lograr solo nuestros objetivos, pero que no están seguras de cuáles son esos objetivos y nos observarán a todos para aprender qué es lo que realmente queremos. Y con suerte, en el proceso, aprenderemos a ser mejores personas. Muchas gracias. (Aplausos) Chris Anderson: Muy interesante, Stuart. Vamos a estar aquí un poco porque creo que están preparando a nuestro próximo orador. Un par de preguntas. La idea de programar ignorancia parece intuitivamente muy poderosa. Al llegar a la superinteligencia, ¿qué puede impedir que un robot lea literatura y descubra esta idea de que el conocimiento es mejor que la ignorancia, cambiando sus propios objetivos y reescribiendo su programación? Stuart Russell: Queremos que aprenda más, como he dicho, sobre nuestros objetivos. Solo ganará seguridad cuanto más acierte. La evidencia estará ahí, y estará diseñado para interpretarla adecuadamente. Comprenderá, por ejemplo, que los libros son muy sesgados en la evidencia que contienen. Solo hablan de reyes y príncipes y hombres blancos poderosos haciendo cosas. Es un problema complicado, pero conforme aprenda más sobre nuestros objetivos será cada vez más útil para nosotros. CA: Y no podría reducirse a una ley, ya sabe, grabada a fuego, "Si un humano alguna vez intenta apagarme yo obedezco, obedezco". SR: Absolutamente no. Sería una idea terrible. Imagine, tiene un auto que se conduce solo y quiere llevar a su hijo de cinco años al jardín de infancia. ¿Quiere que su hijo de cinco años pueda apagar el coche mientras conduce? Probablemente no. Por tanto necesita entender cuán racional y sensata es la persona. Cuanto más racional sea la persona, más dispuesto estará a dejar que lo apaguen. Si la persona es impredecible o incluso malintencionada estará menos dispuesto a permitir que lo apaguen. CA: Stuart, permítame decir que de veras espero que resuelva esto por todos nosotros. Muchas gracias por su charla. Ha sido increíble, gracias. (Aplausos)
TED.com translations are made possible by volunteer translators. Learn more about the Open Translation Project.
© TED Conferences, LLC. All rights reserved.
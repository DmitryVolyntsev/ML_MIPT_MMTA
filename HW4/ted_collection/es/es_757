Hola, soy Joy, una poetisa del código, en una misión para frenar una fuerza invisible que crece, una fuerza que llamo "mirada codificada", mi término para el sesgo algorítmico.
El sesgo algorítmico, como el humano, se traduce en injusticia. Pero, los algoritmos, como los virus, pueden propagar sesgos a gran escala a un ritmo acelerado. El sesgo algorítmico puede también generar experiencias de exclusión y prácticas discriminatorias. Les mostraré lo que quiero decir.
(Video) Joy Buolamwini: Hola, cámara. Tengo una cara. ¿Puedes ver mi cara? ¿Sin lentes? Puedes ver su cara. ¿Qué tal mi cara? Tengo una máscara. ¿Puedes verla?
Joy Buolamwini: ¿Cómo ocurrió esto? ¿Por qué estoy ante una computadora con una máscara blanca, intentando que una cámara barata me detecte? Cuando no lucho contra la mirada codificada como poetisa del código, soy estudiante de posgrado en el Laboratorio de Medios del MIT, y allí puedo trabajar en todo tipo de proyectos caprichosos, incluso el Aspire Mirror, un proyecto que realicé para proyectar máscaras digitales en mi propio reflejo. Entonces, de mañana, si quería sentirme poderosa, podía convertirme en león. Si quería inspiración, podía usar una cita. Entonces, usé el software de reconocimiento facial para crear el sistema, pero me resultó muy difícil probarlo sin colocarme una máscara blanca.
Desafortunadamente, ya tuve este problema antes. Cuando era estudiante de informática en Georgia Tech, solía trabajar con robots sociales, y una de mis tareas fue lograr que un robot jugara a esconderse, un juego de turnos simple donde las personas cubren sus rostros y luego las descubren diciendo: "Aquí está". El problema es que el juego no funciona, si no te pueden ver y el robot no me veía. Pero usé el rostro de mi compañera para terminar el proyecto, entregué la tarea, y pensé que otra persona resolvería este problema.
Al poco tiempo, me encontraba en Hong Kong en una competencia de emprendedores. Los organizadores decidieron llevar a los participantes a un recorrido por empresas locales emergentes. Una de ellas tenía un robot social, y decidieron hacer una demostración. La demostración funcionó bien hasta que llegó mi turno, y probablemente pueden adivinar. No pudo detectar mi rostro. Pregunté a los desarrolladores qué pasaba, y resultó que habíamos usado el mismo software genérico de reconocimiento. Al otro lado del mundo, aprendí que el sesgo algorítmico puede viajar tan rápido como el tiempo que lleva descargar archivos de Internet.
Entonces, ¿qué sucede? ¿Por qué no se detecta mi rostro? Bueno, debemos pensar cómo hacemos que las máquinas vean. La visión por computadora usa técnicas de aprendizaje de máquina para el reconocimiento facial. Se trabaja así, creando una serie de prueba con ejemplos de rostros. Esto es un rostro. Esto es un rostro. Esto no lo es. Con el tiempo, puedes enseñar a una computadora a reconocer rostros. Sin embargo, si las series de prueba no son realmente diversas, todo rostro que se desvíe mucho de la norma establecida será más difícil de detectar, que es lo que me sucedía a mí.
Pero no se preocupen, tengo buenas noticias. Las series de prueba no se materializan de la nada. En verdad las podemos crear. Por ende, se pueden crear series de prueba con espectros completos que reflejen de manera más exhaustiva un retrato de la humanidad.
Ya han visto en mis ejemplos cómo con los robots sociales me enteré de la exclusión por el sesgo algorítmico. Además, el sesgo algorítmico puede generar prácticas discriminatorias. En EE.UU. los departamentos de policía incorporan software de reconocimiento facial en su arsenal para la lucha contra el crimen. Georgetown publicó un informe que muestra que uno de cada dos adultos en EE.UU., 117 millones de personas, tiene sus rostros en redes de reconocimiento facial. Los departamentos de policía hoy tienen acceso a esas redes no reguladas, mediante algoritmos cuya exactitud no ha sido testeada. Sabemos que el reconocimiento facial no es a prueba de fallas y etiquetar rostros de forma consistente aún es un desafío. Tal vez lo han visto en Facebook. Mis amigos y yo nos reímos, cuando vemos a otros mal etiquetados en nuestras fotos. Pero identificar mal a un sospechoso no es un tema para reírse, tampoco lo es violar la libertad civil.
El aprendizaje automático se usa para el reconocimiento facial, pero también se está extendiendo al campo de la visión por computadora. En su libro, "Armas de destrucción matemática", la científica de datos Cathy O'Neil habla sobre los nuevos WMDs, algoritmos amplios, misteriosos y destructivos que se usan cada vez más para tomar decisiones que influyen sobre muchos aspectos de nuestras vidas. ¿A quién se contrata o se despide? ¿Recibes el préstamo? ¿Y la cobertura de seguros? ¿Eres aceptado en la universidad a la que deseas entrar? ¿Tú y yo pagamos el mismo precio por el mismo producto comprado en la misma plataforma?
La aplicación de la ley también empieza a usar el aprendizaje de máquina para la predicción de la policía. Algunos jueces usan puntajes de riesgo generados por máquinas para determinar cuánto tiempo un individuo permanecerá en prisión. Así que hay que pensar sobre estas decisiones. ¿Son justas? Y hemos visto que el sesgo algorítmico no necesariamente lleva siempre a resultados justos.
Entonces, ¿qué podemos hacer al respecto? Bueno, podemos empezar a pensar en cómo creamos un código más inclusivo y emplear prácticas de codificación inclusivas. Realmente empieza con la gente. Con los que codifican cosas. ¿Estamos creando equipos de amplio espectro con diversidad de personas que pueden comprobar los puntos ciegos de los demás? Desde el punto de vista técnico, importa cómo codificamos. ¿Lo gestionamos con equidad al desarrollar los sistemas? Y finalmente, importa por qué codificamos. Hemos usado herramientas informáticas para generar una riqueza inmensa. Ahora tenemos la oportunidad de generar una igualdad aún más grande si hacemos del cambio social una prioridad y no solo un pensamiento. Estos son los tres principios que constituirán el movimiento "codificador". Quién codifica importa, cómo codificamos importa, y por qué codificamos importa.
Así que, para abordar la codificación, podemos empezar a pensar en construir plataformas que puedan identificar sesgos reuniendo experiencias de la gente como las que compartí, pero también auditando el software existente. También podemos crear grupos de formación más inclusivos. Imaginen una campaña de "Selfies por la inclusión" donde Uds. y yo podamos ayudar a los desarrolladores a crear grupos de formación más inclusivos. Y también podemos empezar a pensar más concienzudamente sobre el impacto social de la tecnología que estamos desarrollando.
Para iniciar el movimiento de codificación, creé la Liga de la Justicia algorítmica, donde todo el que se preocupa por la equidad puede ayudar a combatir la mirada codificada. En codedgaze.com pueden informar sesgos, solicitar auditorías, convertirse en un betatesters y unirse a la conversación en curso, #codedgaze.
Así que los invito a que se unan a mí para crear un mundo donde la tecnología trabaje para todos nosotros, no solo para algunos de nosotros, un mundo donde se valore la inclusión y así centrar el cambio social.
Gracias.
(Aplausos)
Pero tengo una pregunta: ¿Se unirán a mí en mi lucha?
(Risas)
(Aplausos)
TED.com translations are made possible by volunteer translators. Learn more about the Open Translation Project.
© TED Conferences, LLC. All rights reserved.
Las emociones influyen en cada aspecto de nuestras vidas, de la salud y el aprendizaje, a la forma de hacer negocios y tomar decisiones, grandes y pequeñas. Las emociones influyen en la forma en la cual interaccionamos entre nosotros. Hemos evolucionado para vivir en un mundo como este, pero en cambio, vivimos la vida cada vez más de esta manera —este es el mensaje de texto que recibí de mi hija anoche— en un mundo desprovisto de emoción. Mi misión es cambiar esto. Quiero devolver las emociones a nuestra experiencia digital.
Empecé con esto hace 15 años. Era ingeniera informática en Egipto y fui aceptada en un programa de doctorado en la Universidad de Cambridge. E hice algo bastante inusual para una joven recién casada, egipcia y musulmana: con el apoyo de mi marido, que debía quedarse en Egipto, hice las maletas y me mudé a Inglaterra. En Cambridge, a miles de kilómetros de casa, me di cuenta de que estaba pasando más horas con mi laptop que con otros seres humanos. Pero a pesar de esta intimidad, mi laptop no tenía ni idea de mi estado de ánimo. No tenía idea de si yo era feliz, si tenía un mal día, o estaba estresada o confundida, y eso era frustrante. Aun peor, cuando me comunicaba en línea con mi familia en casa, sentía que todas mis emociones desaparecían en el ciberespacio. Sentía nostalgia, estaba sola, y algunos días lloraba, pero todo lo que tenía para comunicar mis emociones era esto. (Risas) Hoy la tecnología es inteligente pero no emocional mucha inteligencia cognitiva, pero nada de inteligencia emocional. Eso me hizo pensar, ¿y si la tecnología pudiera interpretar nuestras emociones? ¿Y si nuestros dispositivos pudieran detectar y reaccionar en consecuencia, como lo harían los amigos con inteligencia emocional? Esas preguntas me guiaron a mí y a mi equipo a crear tecnologías capaces de leer emociones y responder, y nuestro punto de partida fue el rostro humano.
Nuestro rostro es uno de los canales más poderosos que usamos para comunicar estados sociales y emocionales, todo, del disfrute y la sorpresa, a la empatía y la curiosidad. En la ciencia de las emociones, cada movimiento de cada músculo facial, es una unidad de acción. Por ejemplo, la unidad de acción 12, no es una superproducción de Hollywood, es el tirón de la comisura labial, componente principal de una sonrisa. Intenten todos. Sonriamos. Otro ejemplo es la unidad de acción 4, las líneas de expresión en el entrecejo cuando juntamos las cejas y se forman estos pliegues y arrugas. No nos gustan, pero es una fuerte señal de una emoción negativa. Hay unas 45 unidades de acción, y combinadas expresan cientos de emociones,
Enseñarle a una computadora a leer estas emociones faciales es difícil, porque estas unidades de acción pueden ser rápidas y sutiles, y se combinan de muchas formas. Tomemos por ejemplo la sonrisa genuina y la socarrona. Se parecen pero expresan cosas diferentes. (Risas) La sonrisa genuina es positiva, la sonrisa socarrona a veces es negativa. A veces una mueca puede hacerte célebre. Pero en serio, es importante para una computadora poder notar la diferencia entre las dos expresiones.
¿Cómo hacemos esto? Introducimos en el programa de computación decenas de miles de ejemplos de personas que sonríen de distintas etnias, edades, géneros, y hacemos lo mismo con las sonrisas socarronas. Luego, los algoritmos en aprendizaje automático buscan estas lineas, pliegues y cambios musculares faciales y básicamente aprenden que todas las sonrisas genuinas tienen características comunes mientras que las sonrisas socarronas tienen otras sensiblemente diferentes. Y la próxima vez que vean un nuevo rostro, sabrán que este rostro tiene las mismas características de una sonrisa genuina, y dirán: "Ajá, la reconozco. Esta es la expresión de una sonrisa".
Y la mejor manera de demostrar cómo funciona esta tecnología es con una demo en vivo, para esto necesito un voluntario, preferentemente alguien con un rostro. (Risas) Chloe será nuestra voluntaria de hoy.
En los últimos 5 años, pasamos de ser un proyecto de investigación en el MIT a ser una empresa, donde mi equipo ha trabajado arduamente en esta tecnología, para que funcione fuera del laboratorio. Y la hemos compactado tanto como para que el lector de las emociones funcione en un dispositivo móvil con una cámara, como este iPad. Así que probémosla.
Como pueden ver, el algoritmo detectó el rostro de Chloe, es este cuadro delimitador blanco, que detecta los contornos principales de sus rasgos faciales, sus cejas, sus ojos, su boca y nariz. La pregunta es: ¿puede reconocer su expresión? Vamos a probar la máquina. Ante todo, pon cara de póquer. Sí, genial. (Risas) Y a medida que sonríe —esta es una sonrisa genuina, es genial— pueden ver como aumenta la barra verde. Esa fue una gran sonrisa. ¿Puedes intentar una sonrisa sutil para ver si la computadora la reconoce? También reconoce sonrisas sutiles. Hemos trabajado arduamente para que esto suceda. Luego levanta una ceja, que indica sorpresa. Frunce el ceño, que indica la confusión. Enfurruñate. Sí, perfecto. Estas son diferentes unidades de acción. Hay muchas más. Esta es solo una demo superficial. Llamamos a cada lectura un dato emocional, que luego pueden actuar juntos para crear distintas emociones. A la derecha de la demo, parece que estás feliz. Eso es alegría. Se desata la alegría. Ahora pon cara de disgusto. Trata de recordar qué sentiste cuando Zayn dejó One Direction. (Risas) Sí, arruga la nariz. Genial. La valencia es bastante negativa, por lo que debe haber sido una gran fan. La valencia indica cuán positiva o negativa es una experiencia, y la vinculación indica lo expresiva que es también. Imaginen que Chloe tiene acceso a este contenido emocional en tiempo real, y que puede compartir sus emociones con quien quiere. Gracias. (Aplausos)
Hasta ahora contamos con 12 000 millones de estos indicadores emocionales. Es la base de datos de emociones más grande del mundo. La hemos recopilado a partir de 2,9 millones de rostros en videos, de personas que accedieron a compartir sus emociones con nosotros, de 75 países del mundo. Crece cada día. Me resulta impactante que ahora podamos cuantificar algo tan personal como las emociones, y poder hacerlo a esta escala.
¿Qué hemos aprendido hasta la fecha? Hay diferencias por género. Nuestros datos confirman algo que Uds. ya sospechaban. Las mujeres son más expresivas que los hombres. No solo sonríen más, sus sonrisas duran más, y ahora podemos cuantificar cómo es que hombres y mujeres responden de maneras tan diferentes. Veamos culturalmente: en EE.UU., las mujeres son un 40 % más expresivas que los hombres, pero curiosamente, no vemos diferencia entre hombres y mujeres en el R.U. (Risas) Por edad: las personas de 50 años o más son un 25 % más emotivos que los más jóvenes. Las mujeres de veintipico sonríen mucho más que los hombres de la misma edad, quizá es una necesidad para las citas. Pero quizá lo que más nos sorprende de estos datos es que solemos ser expresivos todo el tiempo, incluso cuando estamos sentados solos frente a nuestros dispositivos y no solo cuando miramos videos de gatos en Facebook. Somos expresivos cuando mandamos emails, mensajes, cuando compramos en línea, o incluso pagando impuestos.
¿Para qué se usan estos datos hoy? Para entender cómo nos relacionamos con los medios, para entender la viralidad y el comportamiento del voto; y también para dar poder dotar de emoción a la tecnología, y quiero compartir algunos ejemplos particularmente especiales para mi. Las gafas portátiles con lector emotivo pueden ayudar a las personas con discapacidad visual a leer los rostros de los demás, y a las personas del espectro autista a interpretar pistas emocionales algo que les cuesta mucho. En educación, imaginen si sus apps educativas detectaran que están confundidos y bajaran la velocidad, o que están aburridos, y aceleraran, como haría un buen profesor en el aula. Y si una pulsera leyera su estado anímico, o el auto detectara que están cansados, o quizá si el frigorífico supiera que están estresados, y se autobloqueara para evitar atracones. (Risas) Me gustaría eso, sí. ¿Y si, cuando estuve en Cambridge, hubiera tenido acceso en tiempo real a mi contenido emocional y hubiera podido compartirlo con mi familia en casa de manera muy natural, como si estuviéramos en la misma habitación juntos?
Creo que dentro de 5 años, todos los dispositivos tendrán un chip lector de emociones y no recordaremos cómo era no poder fruncir el ceño a nuestro dispositivo y que nuestro dispositivo dijera: "Mmm, no te gusta, ¿no?" Nuestro desafío más grande es que hay tantas aplicaciones para esta tecnología, que mi equipo y yo nos dimos cuenta de que no podemos con todo solos, por eso liberamos esta tecnología para que otros desarrolladores puedan desarrollarla y ser creativos. Reconocemos que hay riesgos potenciales y potencial para el abuso, pero en mi opinión, habiendo pasado muchos años haciendo esto, creo que los beneficios para la humanidad de contar con tecnología emocionalmente inteligente superan con creces las desventajas por uso indebido. Y los invito a todos a tomar parte en el debate. Cuantas más personas conozcan esta tecnología, más podemos decir sobre cómo se usa. Conforme nuestras vidas se vuelven cada vez más digitales, estamos librando una batalla perdida tratando de evitar los dispositivos para recuperar nuestras emociones. Por eso yo propongo, en cambio, incorporar las emociones a la tecnología y hacer que nuestras tecnologías sean más receptivas. Quiero que esos dispositivos que nos han separado nos vuelvan a unir. Y humanizando la tecnología, tenemos esta oportunidad excelente de reinventar la manera de conectarnos con las máquinas, y por lo tanto, la manera de como nosotros, los seres humanos, conectamos unos con otros.
Gracias.
(Aplausos)
TED.com translations are made possible by volunteer translators. Learn more about the Open Translation Project.
© TED Conferences, LLC. All rights reserved.
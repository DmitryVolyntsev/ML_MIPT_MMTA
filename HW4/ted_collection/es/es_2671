Les mostraré algo.
(Video) Niña: Eso es un gato sentado en una cama. El niño está acariciando al elefante. Esas son personas que van en un avión. Ese es un avión grande.
Fei-Fei Li: Así describe una niña de 3 años lo que ve en una serie de fotos. Tal vez le falta mucho por aprender sobre este mundo, pero ya es experta en algo importante: entender lo que ve. Tecnológicamente, nuestra sociedad está más avanzada que nunca. Enviamos personas a la luna, nuestros teléfonos nos hablan o personalizan radios para reproducir solo la música que nos gusta. Sin embargo, nuestras máquinas y computadoras más avanzadas aún tienen problemas en ese aspecto. Hoy estoy aquí para darles un reporte de nuestros últimos avances en visión artificial, una de las tecnologías potencialmente más revolucionarias en la ciencia de la computación.
Es cierto, hemos inventado autos que conducen solos, pero sin una visión inteligente, realmente no pueden distinguir entre una bolsa arrugada de papel en el camino, que puede uno pisar, y una roca del mismo tamaño, que debemos evitar. Hemos creado fabulosas cámaras de muchos megapíxeles, pero aún no podemos devolverle la vista a un ciego. Los drones pueden volar sobre grandes superficies de tierra, pero no tienen tecnología de visión suficiente para ayudarnos a monitorear los cambios en los bosques tropicales. Hay cámaras de seguridad en todas partes, pero no nos alertan cuando un niño se está ahogando en una piscina. Las fotos y los videos se están volviendo parte integral de la vida global. Se generan a un ritmo mucho mayor de lo que cualquier humano, o equipo de humanos, podría ver, y Uds. y yo contribuimos a eso en este TED. Aun así, nuestro software más avanzado tiene problemas para entender y gestionar este enorme contenido. En otras palabras, colectivamente como una sociedad, somos muy ciegos, porque nuestras máquinas más inteligentes aún son ciegas.
Se preguntarán: "¿Por qué es tan difícil?" Las cámaras pueden tomar fotos como esta convirtiendo luz en matrices numéricas bidimensionales conocidas como pixeles, pero estos son solo números vacíos. En sí mismos no tienen significado. Al igual que oír no es lo mismo que escuchar, tomar fotografías no es lo mismo que ver; y solo viendo podemos realmente entender. De hecho, le tomó a la Madre Naturaleza 540 millones de años de arduo trabajo lograr esta tarea, y mucho de ese esfuerzo consistió en desarrollar el sistema de procesamiento visual en el cerebro, no los ojos en sí. La visión empieza en los ojos, pero, en realidad, ocurre en nuestro cerebro.
Durante 15 años, empezando desde mi doctorado en Caltech y luego al frente del laboratorio Stanford Vision Lab, he trabajado con mis mentores, colaboradores y estudiantes para enseñar a las computadoras a ver. Nuestro campo de investigación se llama "visión artificial y aprendizaje automático". Es parte del campo de la inteligencia artificial. Queremos enseñar a las máquinas a ver tal como nosotros lo hacemos: nombrar objetos, identificar personas, inferir la geometría 3D de las cosas, entender relaciones, emociones, acciones e intenciones. Nosotros tejemos historias completas de la gente, los lugares y las cosas simplemente con mirarlas.
El primer paso hacia esta meta es enseñar a una computadora a ver objetos, la unidad básica del mundo visual. En términos más simples, imaginen este proceso mostrando a las computadoras algunas imágenes de entrenamiento de un objeto en particular, digamos gatos, y diseñar un modelo que aprenda de estas imágenes. ¿Qué tan difícil puede ser esto? A fin de cuentas, un gato es solo un conjunto de formas y colores, y eso fue lo que hacíamos en los inicios de la modelización de objetos. Decíamos al algoritmo de la computadora en un lenguaje matemático que un gato tiene cara redonda, cuerpo regordete, dos orejas puntiagudas y cola larga, y así quedaba bien. Pero ¿qué me dicen de este gato? (Risas) Está todo retorcido. Se debe agregar otra figura y otra perspectiva al modelo del objeto. ¿Y si los gatos están escondidos? ¿Qué tal estos gatos tontos? Ahora entienden mi idea. Incluso algo tan simple como una mascota puede tener un número infinito de variaciones en el modelo del objeto, y eso es solo un objeto.
Así que hace unos 8 años, una observación simple y profunda cambió mi perspectiva. Nadie le dice al niño cómo ver, menos aún en los primeros años. Ellos aprenden a través de ejemplos y experiencias del mundo real. Si consideramos los ojos de un niño como un par de cámaras biológicas, toman una foto cada 200 milisegundos, el tiempo promedio en que el ojo hace un movimiento. Entonces, a los 3 años un niño ha visto cientos de millones de fotografías del mundo real. Esos son muchos ejemplares de entrenamiento. Así que en lugar de enfocarnos solo en mejorar los algoritmos, mi intención fue dotar a los algoritmos con los datos de entrenamiento que un niño adquiere con la experiencia tanto en cantidad como en calidad.
Al conocer esto supimos que necesitábamos recolectar muchas más imágenes que nunca, tal vez miles de veces más; y junto con el profesor Kai Li en la Universidad de Princeton, lanzamos el proyecto ImageNet en 2007. Por suerte, no tuvimos que ponernos una cámara en la cabeza y esperar muchos años. Entramos a Internet, el banco de imágenes más grande creado por la humanidad. Descargamos casi 1000 millones de imágenes y usamos tecnología de crowdsourcing como la plataforma Amazon Mechanical Turk para etiquetar estas imágenes. En su mejor momento, ImageNet fue uno de los empleadores más importantes de trabajadores en Amazon Mechanical Turk: Casi 50 000 trabajadores de 167 países del mundo nos ayudaron a limpiar, separar y etiquetar casi 1000 millones de imágenes candidatas. Se necesitó todo ese esfuerzo para capturar apenas una fracción de todas las imágenes que un niño asimila en sus primeros años de desarrollo.
Viendo en retrospectiva, esta idea de usar muchos datos para entrenar algoritmos puede parecer obvia ahora. Sin embargo, en 2007 no era tan evidente. Estuvimos solos en este viaje por un buen rato. Algunos colegas me sugerían hacer algo más útil para mi cátedra, y con frecuencia teníamos problemas para conseguir financiamiento. Incluso llegué a decir a mis alumnos, como broma, que tendría que reabrir mi tintorería para financiar ImageNet. Después de todo, así fue como financié mis años de universidad.
Seguimos adelante. En 2009, el proyecto ImageNet juntó una base de datos con 15 millones de imágenes de 22 000 tipos de objetos organizados por palabra en inglés de uso cotidiano. En cantidad y calidad, tuvieron una escala sin precedentes. Por ejemplo, en el caso de los gatos, tenemos más de 62 000 gatos con todo tipo de apariencias y poses y todo tipo de gatos domésticos y salvajes. Estábamos entusiasmados por haber creado ImageNet y queríamos que todo el mundo de la investigación se beneficiara, así que, al estilo TED, abrimos toda la base de datos a la comunidad mundial de investigadores de forma gratuita. (Aplausos)
Ahora que tenemos los datos para nutrir el cerebro de nuestra computadora, estamos listos para volver a los algoritmos. La abundancia de información aportada por ImageNet fue el complemento perfecto para un tipo particular de algoritmos de aprendizaje automático llamado red neuronal convolucional, ideado por Kunihiko Fukushima, Geoff Hinton y Yann LeCun en los años 70 y 80. Como el cerebro que tiene miles de millones de neuronas muy bien conectadas, la unidad operativa fundamental en una red neuronal es un nodo con forma de neurona. Toma datos de otros nodos los procesa y los manda a otros nodos. Además, estos cientos de miles o incluso millones de nodos se organizan en capas jerárquicas, algo parecido al cerebro. En una red neuronal típica que usamos para entrenar nuestro modelo de reconocimiento de objetos hay 24 millones de nodos, 140 millones de parámetros y 15 000 millones de conexiones. Es un modelo enorme. Alimentado por la información masiva de ImageNet y las CPUs y GPUs modernas que entrenan este inmenso modelo, la red neuronal convolucional tuvo un éxito inesperado. Se volvió la ingeniería ganadora para generar nuevos y emocionantes resultados en reconocimiento de objetos. Esta es una computadora que nos dice que la foto tiene un gato y dónde está el gato. Desde luego hay más cosas aparte de los gatos así que hay un algoritmo informático que nos dice que hay un niño y un oso de peluche en la foto; un perro, una persona y un papalote al fondo; o una foto de cosas muy ocupadas como un hombre, una patineta, un barandal, una lámpara etc. A veces, cuando la computadora no está segura de lo que ve, le hemos enseñado a darnos una respuesta segura en lugar de comprometer su respuesta, tal como lo haríamos nosotros. Pero otras veces nuestro algoritmo informático es muy acertado al decirnos qué son los objetos exactamente, como la marca, modelo y año de los coches.
Aplicamos este algoritmo a millones de imágenes de Google Street View de cientos de ciudades de Estados Unidos y hemos aprendido algo muy interesante: primero, confirmó nuestra idea de que los precios de los autos se relacionan bien con los ingresos del hogar. Pero sorprendentemente, los precios de los autos se relacionan también con las tasas de criminalidad en la ciudades o los patrones de votación por código postal.
Un minuto. ¿Eso es todo? ¿Acaso la computadora ya sobrepasó las capacidades humanas? No tan rápido. Hasta ahora solo hemos enseñado a la computadora a ver objetos. Es como un niño pequeño que aprende a decir palabras. Es un logro increíble, pero es apenas el primer paso. Pronto daremos otro paso y los niños empiezan a comunicarse con frases. Así que en lugar de decir que hay un gato en la foto, la niña ya dice que el gato está sobre la cama.
Así que para enseñar a una computadora a ver una foto y generar frases la conjunción de mucha información y el algoritmo de aprendizaje automático debe dar otro paso. Ahora, la computadora tiene que aprender de fotografías así como de frases en lenguaje natural generado por humanos. De la forma en que el cerebro integra visión y lenguaje, desarrollamos un modelo que conecta partes de cosas visuales como fragmentos visuales con palabras y frases en oraciones.
Hace unos 4 meses finalmente juntamos todo esto y produjimos uno de los primeros modelos de visión artificial que puede generar frases como las de un humano cuando ve una foto por primera vez. Ahora estoy lista para mostrarles lo que dice la computadora cuando ve la fotografía que la niña vio al inicio de esta charla.
(Video) Computadora: Un hombre está junto a un elefante. Un avión grande está encima de una pista de aeropuerto.
FFL: Desde luego, seguimos trabajando para mejorar los algoritmos y aún tiene mucho que aprender. (Aplausos)
Y la computadora aún comete errores.
(Video) Computadora: Un gato recostado en la cama en una sábana.
FFL: Y cuando ha visto demasiados gatos, cree que todo lo que ve parece un gato.
(Video) Computadora: Un niño tiene un bate de béisbol. (Risas)
FFL: O si nunca ha visto un cepillo de dientes, lo confunde con un bate de béisbol.
(Video) Computadora: Un hombre montando un caballo junto a un edificio. (Risas)
FFL: No le hemos enseñado arte elemental a las computadoras.
(Video) Computadora: Una cebra en un campo de hierba.
FFL: Y no ha aprendido a apreciar la belleza deslumbrante de la naturaleza, como lo hacemos nosotros.
Así que ha sido un largo camino. Pasar de los 0 a los 3 años fue difícil. El verdadero reto es llegar a los 13 y mucho más todavía. Recordemos nuevamente esta foto del niño y el pastel. Hasta ahora, le hemos enseñado a la computadora a ver objetos o incluso darnos una pequeña historia cuando ve la foto.
(Video) Computadora: Una persona sentada a la mesa con un pastel.
FFL: Pero hay mucho más en esta fotografía que simplemente una persona y un pastel. Lo que la computadora no ve es que este es un pastel especial italiano exclusivo de Pascua. El niño viste su camiseta favorita, que le regaló su papá tras un viaje a Sídney, y nosotros podemos decir qué tan feliz está y qué pasa por su mente en ese momento.
Ese es mi hijo Leo. En mi búsqueda de inteligencia visual, pienso constantemente en él y en el futuro en que va a vivir. Cuando las máquinas puedan ver, los médicos y enfermeras tendrán un par extra de ojos incansables para ayudarlos a diagnosticar y cuidar de los pacientes. Los autos andarán de forma inteligente y segura en los caminos. Robots, y no solo humanos, nos ayudarán a desafiar zonas de desastre para salvar heridos y atrapados. Descubriremos nuevas especies, mejores materiales, y exploraremos fronteras nunca vistas con ayuda de las máquinas.
Poco a poco, damos a las máquinas el don de la vista. Primero les enseñamos a ver. Luego ellas nos ayudarán a ver mejor. Por primera vez, los ojos humanos no serán los únicos que exploren nuestro mundo. No solo usaremos máquinas por su inteligencia, también colaboraremos con ellas de formas que ni siquiera imaginamos.
Esta es mi misión: dar a las computadoras inteligencia visual y crear un mejor futuro para Leo y para el mundo.
Gracias.
(Aplausos)
TED.com translations are made possible by volunteer translators. Learn more about the Open Translation Project.
© TED Conferences, LLC. All rights reserved.